
To verify if running all Tribler functionality on mobile devices is feasible we take a look at relevant performance characteristics and take several measurements.
For scale a laptop and old computer are included in some measurements.
We focus on the design aspects and evaluate per feature / key performance indicator.

% Rationale
% Metrics
% Expected / desired results
% Setup
% Results
% Conclusions

\section{API latency}
% Rationale
The user expects operations to take a consistent and reasonable amount of time.
By design all functionality is going through the API.
% Metrics
We use Apache JMeter to verify if the API is responding consistently within a reasonable amount of time by measuring the latency.
JMeter measures the latency from just before sending the request to just after the first response has been received. \cite{jmeter_glossary}
Thus the measurement includes the time needed to assemble the request as well as processing it and returning a response with latency on the way back.
It excludes the transfer time of the complete response and subsequent processing and rendering time.
Any client can process and render a response differently, possibly in a streaming fashion.
By leaving out that element this metric is measuring the operation time via the API.
% Expected / desired results
We want to see that the latency is bounded, consistent and generally low.
Previous research has shown the following results on a computer:
%FIG api_laurens
% Setup
We did the same benchmark with a slightly different setup.
A Nexus 6 smart-phone with Android 6.0.1 Cyanogen mod was connected via usb to a laptop running JMeter.
The API tcp-port was forwarded with ADB.
With JMeter we request the discovered channels from the API a 1000 times with 300 requests per minute and measure the latency.
The requested data is not equal to the data used in figure \ref{fig:ap_laurens}.


% Results
The following figure shows the latency for every request.

1 graph
1 device

% Conclusions
The results shows large spikes and an unexpected pattern.
It turns out this approach still suffers from performance issues due to imperfect multi-threaded coding of Tribler.
Looks like more things are at play here.
CPU freq?
Create and show laurens' histogram from his data. difference!


\section{Testing and coverage}
% Rationale
The design choice of reusing all Tribler core source code means we need to verify its correctness.
To make sure all code on Android works the same as on other supported platforms we need to test all code.
Tribler has some unit tests and integration tests that cover a large portion of the code, but not all.
% Metrics
% Expected / desired results
% Setup
The nosetests module together with the coverage module are used to evaluate this.
A nexus 

% Results
The followinng table shows the results of two times.

1 table
1 device

1.
testsuite name="nosetests" tests="711" errors="14" failures="13" skip="30"
coverage branch-rate="0" line-rate="0.7241" timestamp="1468681674588" version="4.1"
2.
testsuite name="nosetests" tests="749" errors="12" failures="15" skip="3"
coverage branch-rate="0" line-rate="0.7861" timestamp="1478016138747" version="4.1"

% Conclusions







Profiler
% Rationale
% Metrics
% Expected / desired results
% Setup

1 graph
1 device
wall clock time
10 minutes run

% Results
Results as expected: crypto lets other things wait, significant part of wall-clock-time.

% Conclusions


Startup time
% Rationale
% Metrics
% Expected / desired results
% Setup
% Results
% Conclusions
1 graph
5 devices


Multichain
% Rationale
% Metrics
% Expected / desired results
% Setup
% Results
% Conclusions
4 graphs
5 devices

without clearing the database, running the experiment a second time shows a significant delay on all devices.

The performance of Multichain:
% block creation graph
3 plots van onderling vergelijkbare resltaten
waarom moet dit experiment er zijn?
Om mobiele devices met Tribler volwaardige nodes in het netwerk te maken moeten ze multichain aankunnen ook na 10 jaar draaien.
Load test, simulatie 10 jaar. (174 days!)
