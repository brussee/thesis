
To verify if running all Tribler functionality on mobile devices is feasible we take a look at relevant performance characteristics and take several measurements.
For scale a laptop and old computer are included in some measurements.
We focus on the design aspects and evaluate per feature / key performance indicator.

% Rationale
% Metrics
% Expected / desired results
% Setup
% Results
% Conclusions

\section{API latency}
% Rationale
The user expects operations to take a consistent and reasonable amount of time.
By design all functionality is going through the API.
% Metrics
We use Apache JMeter to verify if the API is responding consistently within a reasonable amount of time by measuring the latency.
JMeter measures the latency from just before sending the request to just after the first response has been received. \cite{jmeter_glossary}
Thus the measurement includes the time needed to assemble the request as well as processing it and returning a response with latency on the way back.
It excludes the transfer time of the complete response and subsequent processing and rendering time.
Any client can process and render a response differently, possibly in a streaming fashion.
By leaving out that element this metric is measuring the operation time via the API.
% Expected / desired results
We want to see that the latency is bounded, consistent and generally low.
Previous research has shown the following results on a computer:
%FIG api_laurens
% Setup
We did the same benchmark with a slightly different setup.
A Nexus 6 smart-phone with Android 6.0.1 Cyanogen mod was connected via USB to a laptop running JMeter.
The API tcp-port was forwarded with ADB.
With JMeter we request the discovered channels from the API a 1000 times and measure the latency.
JMeter is also capable of setting a desired number of requests per minute, provided the device can handle it.
We set this parameter to 300 requests per minute, equal to the other benchmark.
% Results
Our database to be queried is not equal to one used in figure \ref{fig:api_laurens}.
The following figure shows the latency for every request.
%FIG api_benchmark
The plot shows large spikes and seemingly a curious and unexpected pattern.
Table \ref{table:api_benchmark} shows the large difference in statistics.
\begin{table}[h]
  \begin{tabular}{l | *{8}{c}}
  	 & N & \overline{x} (ms) & Min. (ms) & Max. (ms) & Ïƒ (ms) & Throughput & KB/second & Avg. Bytes \\ \hline
	Nexus 6 & 1000 & 1803 & 1292 & 3292 & 301.25 & 33.3 req/minute & 347.44 & 641709.0 \\ \hline
	PC         & 1000 & 5        & 3      & 107   & 4.42     & 140.8 req/second & 2548.00 & 18525.0 \\ \hline
  \end{tabular}
  \caption[Total response time statistics]{These numbers are based on elapsed time, not latency, from the entire benchmark.}
  \label{table:api_benchmark}
\end{table}
The set rate of 300 requests per minute is clearly not reached by the smart-phone in our benchmark by a factor of 9.
The difference in amount of response data turned out to be much bigger than expected with a factor of 35.
% Conclusions
Considering the difference in amount of response data and the bandwidth of USB versus internal memory it is an unfair comparison.
However, based on our result we may conclude that the API does not respond consistently and appear to behave differently than the other benchmark.
The device not being able to process 5 requests per second possibly reveals information about the connection bandwidth and the multi-threaded processing capability.
We assume that the 480MB/s theoretical bandwidth of USB2.0 is not a bottleneck considering the 2,5 MB/s result of the PC.
Being a mobile device also other aspects my be at play here, like CPU frequency scaling.
However this was turned off by acquiring a wake lock from the Android OS.
It appears then that our design approach still suffers from performance issues due to imperfect multi-threaded performance.
Because if the CPU is simply not powerful enough we expect to see a linear pattern instead of what we actually see in \ref{fig:api_benchmark}.


\section{Testing and coverage}
% Rationale
The design choice of reusing all Tribler core source code means we need to verify its correctness.
To make sure all code on Android works the same as on other supported platforms we need to test all code.
Tribler has some unit tests and integration tests that cover a large portion of the code, but not all.
% Metrics
The ratio of tested lines of code with respect to the total number of lines of code is the coverage line-rate.
% Expected / desired results
We expect to see a line-rate value close to 1, but not 1 since we know the tests do not cover everything.
% Setup
All tests were run two times on the same device with 11 weeks of development in between.
The nose module was used for running the tests together with the coverage module for gathering coverage data.
The same Nexus 6 smart-phone with Android 6.0.1 Cyanogen mod was used in both runs.
% Results
The following table shows the results of both executions.
\begin{table}[h]
	\begin{tabular}{l | *{5}{c}}
		Run & Tests & Errors & Failures & Skipped & Line-rate \\ \hline
		1     & 711   & 14       & 13          & 30          & 0.7241 \\ \hline
		2     & 749   & 12       & 15          & 3            & 0.7861 \\ \hline
	\end{tabular}
	\caption[Total response time statistics]{These numbers are based on elapsed time, not latency, from the entire benchmark.}
	\label{table:testing_coverage}
\end{table}
The number of tests has increased as well as the coverage line-rate while the number of errors and skipped tests have decreased.
A failure means an assertion was not met and an error represents an exception while running a test.
% Conclusions
Therefore seeing that the number of failures increased is not that bad since the number of errors decreased.
If the line-rate is 1 you still need the the branch-rate to be 1 as well before you can be confident the code will work as expected.
The branch-rate is the number of code paths tested with respect to the total number of code paths possible.
Unfortunately this metric is not part of the current test plan.
Nevertheless the metrics show improvement overall.


\section{Profiling}
% Rationale
Because of the challenges put forward in chapter \ref{ch:tribler_mobile} we investigate if time is spent disproportionately on some function.
% Expected / desired results
We expect that the limited resources of a mobile device may impact particular features more than others.
If hardware acceleration is not present the less powerful CPU may struggle with encryption tasks.
% Metrics
Instead of CPU time, time actually spent processing by the CPU, we measure wall-clock time.
This way we measure the amount of time a user would have to wait for a certain function to be executed.
% Setup
With the cProfile Python module and the visualisation tool SnakeViz we can see if any function takes a disproportionate amount of time.
A Nexus 6 smart-phone with Android 6.0.1 Cyanogen mod was used for this.
The profiler was running for 10 minutes with Tribler during normal operation and without any user input.
% Results
%FIG profile_1468515157-2
Figure \ref{fig:profile} shows that 27\% of the time is spent on verifying cryptographic signatures.
The bright pink represents the update function, which signifies various business logic upon receiving a torrent.
Also notable is the same stack of functions within and outside of a community on top of the wrapper.
This can be explained by the fact that torrents can be discovered outside of a community too.
% Conclusions
The significant chunk of time that the crypto takes is as expected.
Since this task is actually delegated to the C library M2Crypto it should be possible to release the GIL of the Python interpreter so other Python code that does not depend on it can be executed.


\section{Startup time}
% Rationale
One of the design objectives of separating the front-end from the back-end was responsiveness.
If an user decides to use Tribler we want to put no obstacle in the way.
Therefore the service starts up in the background, separately from the GUI.
However before any task can be executed by the service it needs to be actually started.
% Metrics
To measure the time it takes Tribler to fully start we measure the time from starting the app up to the moment the Tribler-started-event is fired.
This events is sent over the API event-stream and signifies that Tribler is fully started and ready to accept all incoming requests.
% Expected / desired results
We expect consistent loading times because right after starting we shut Tribler down again.
% Setup
To get a good idea of how the user experience may differ we measure the startup time 10 times on 5 different devices.
% Results
%FIG startup_time
Table \ref{table:startup_time} shows the statistics per device.
\begin{table}[h]
	\begin{tabular}{l | *{5}{c}}
		Device & N & \overline{x} (s) & Min. (s) & Max. (s) & s (s) \\ \hline
		Nexus 10        & 10 & \\ \hline
		Nexus 6          & 10 & 4.319 & 4.124 & 4.670 & 0.179 \\ \hline
		Nexus 5          & 10 & 3.353 & 3.273 & 3.459 & 0.081 \\ \hline
		Galaxy Nexus & 10 & 7.086 & 6.161 & 7.772 & 0.454 \\ \hline
		S3                   & 10 & 31.935 & 30.616 & 33.940 & 1.116 \\ \hline
	\end{tabular}
	\caption[Total response time statistics]{These numbers are based on elapsed time, not latency, from the entire benchmark.}
	\label{table:starup_time}
\end{table}
The results show a very small sample standard deviation and a very low startup time.
The S3 is performing worse than may be expected judging from the results of the other devices.
% Conclusions
The reason for that may be that this phone was not wiped and given a fresh install of Android.
That could mean that other applications installed on a device could significantly impact the performance of Tribler.
The sample standard deviation is relatively small though, which contradicts this hypothesis.
This should be investigated further including if anything can be done on the part of Tribler.


Multichain
% Rationale
% Metrics
% Expected / desired results
% Setup
% Results
% Conclusions
4 graphs
5 devices

without clearing the database, running the experiment a second time shows a significant delay on all devices.

The performance of Multichain:
% block creation graph
3 plots van onderling vergelijkbare resltaten
waarom moet dit experiment er zijn?
Om mobiele devices met Tribler volwaardige nodes in het netwerk te maken moeten ze multichain aankunnen ook na 10 jaar draaien.
Load test, simulatie 10 jaar. (174 days!)
